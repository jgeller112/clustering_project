---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Clustering

## Introduction

In this task, individuals heard speech tokens from a number of different speakers and freely classified them into groups. Based on previous work, I used hierarchical clustering to examine what natural clusters or groups formed as the result of this free classification. 

```{r}
# For reproducibility
set.seed(666)
```


```{r}
library(here)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
library(fpc) # kmeans clustering

```

## Data Preparation

1. I wrangled the DF so that each row corresponds to each talker and each column corresponds to each participant. 
2. I removed missing data. 
3. I did not standardize the data. 

### Read in the data

```{r}

clust_data <- read_csv(here("data", "class_wide_1.csv")) # read in data

clust_data <- select(clust_data, -X1, -`54`) # remove extra col sub 54 has weird formatting

clust_data <- as.data.frame(clust_data) # turn into df 

rownames(clust_data) <- clust_data$speaker # make row names speaker

clust_data <- select(clust_data,-speaker) # remove extra col sub 54 has weird formatting

head(clust_data)# show first couple rows

```

## Agglomerative Hierarchical Clustering

I am going to cluster the data using average link clustering. Average link clustering computes all pairwise dissimilarities between the elements, and considers the average of these dissimilarities as the distance between clusters. 

1. I calculate the dissimilarity matrix using euclidean distance. 

2. I compute the clustering with average link. 

3. I plot the cluster solution

```{r}
# Dissimilarity matrix
d <- dist(clust_data, method = "euclidean")

# Hierarchical clustering using Average Linkage
hc1 <- hclust(d, method = "average" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)

```

### How Many Clusters?

In the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.

The height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.

Although hierarchical clustering provides a fully connected dendrogram representing the cluster relationships, you may still need to choose the preferred number of clusters to extract. Fortunately we can execute approaches similar to k-means clustering. The following compares results provided by the elbow, silhouette, and gap statistic methods. There is no definitively clear optimal number of clusters in this case; although, the silhouette method and Elbow method suggests anywhere between 2-5 clusters. 

Humans cant live with this ambiguity. Let's use k-means clustering to determine the number of clusters we should use. 


```{r}
# Plot cluster results
p1 <- fviz_nbclust(clust_data, FUN = hcut, method = "wss", 
                   k.max = 10) +
  ggtitle("(A) Elbow method")
p2 <- fviz_nbclust(clust_data, FUN = hcut, method = "silhouette", 
                   k.max = 10) +
  ggtitle("(B) Silhouette method")
p3 <- fviz_nbclust(clust_data, FUN = hcut, method = "gap_stat", 
                   k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)

ggsave("HCstats.png", width=10, height=8)

```

## K-means

K-means is another type of clustering algorithm. For a more objective way to determine how many clusters there are, I am going to run k-means clustering over a range of cluster values (here 3-10 clusters). I will use the `fpc` package and the `kmeansrun` function. This function iterates over a number of clusters and chooses the best number of clusters. 

```{r}
#run kmeans over a number of ranges (3:10) here

cl <- kmeansruns(clust_data, krange = 2:10, iter.max = 1000)

# pick the best one
cl$bestk

```

The k-means analysis suggests 2 clusters is best. I personally think 3 clusters better represents the data. It is really a subjective call on your part. Let's visualize both to see what the clusters look like.

## Visualize Clusters

### Dendogram

#### 2 clusters

Here is a dendogram cut at 2. 

```{r}

hc.cut <- hcut(clust_data, k = 2, hc_method = "average")

fviz_dend(hc.cut, show_labels = TRUE, rect = TRUE)


ggsave("dendogram2.png", width=10, height=8, dpi=700)

```

Here is a dendogram cut at 3. 

```{r}

hc.cut <- hcut(clust_data, k = 3, hc_method = "average")

fviz_dend(hc.cut, show_labels = TRUE, rect = TRUE)


ggsave("dendogram3.png", width=10, height=8, dpi=700)

```

#### 2 Clusters

Let's visualize the clusters in two dimensions as it is a bit easier to read that the above dendrogram. I saved this cluster figure as "2clust.png."

```{r}
# Cut tree into 3 groups
sub_grp <- cutree(hc.cut, k = 2)

# Number of members in each cluster
sub_grp

s=fviz_cluster(list(data = clust_data, cluster = sub_grp), labelsize = 10, repel = TRUE)

s

ggsave("2clust.png", width=10, height=8, dpi=700)
```


#### 3 Clusters

Let's visualize the clusters in two dimensions as it is a bit easier to read that the above dendrogram. I saved this cluster figure as "3clust.png." I also saved the data with the cluster number of each speech token as "speech_group.csv." With this you can visualize the clusters how you want. 

```{r}
# Cut tree into 3 groups
sub_grp <- cutree(hc.cut, k = 3)

# Number of members in each cluster
sub_grp

fviz_cluster(list(data = clust_data, cluster = sub_grp))

ggsave("3clust.png", width=10, height=8, dpi=700)
```

# Conclusion

From this, we glean that two clusters are adequate. 

I think 3 better represents the data, however. 

- Cluster 1: English/African

- Cluster 2: Indo/European

- Cluster 3: Asian

Just to summarize, I ran a hierarchical clustering analysis using the average link method to classify talkers in a free classification task. Because there was some ambiguity in terms of the correct correct number of clusters, I ran an iterative k-means analysis ranging from  two clusters to ten cluster. This analysis suggested we should use two clusters. If you think three clusters better represents the data please use three instead. 



