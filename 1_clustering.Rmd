---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Clustering

## Introduction

In this task, individuals heard spoken speech tokens and freely classified them into groups. Using hierarchical clustering we aimed to see what clusters or groups appear as a result of the free classification task. 

45 speech samples were selected from The Speech Accent Archive. The talkers included three American English regional dialects, three international English dialects, and nine nonnative accents. The nonnative accents were split into three accents from East Asia, three accents from South Asia, and three accents from Southeast Asia. The American English dialects included the New England dialect, the Southern dialect, and the Midland dialect. The international English dialects included British English, Australian English, and Africaans. The native languages of the nonnative-accented talkers were Mandarin, Korean, and Japanese from East Asia, Bengali, Gujarati, and Urdu from South Asia, and Indonesian, Tagalog, and Thai from Southeast Asia. 

```{r}
library(here)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms

```

### Data Preparation

1. Rows are observations (individuals) and columns are variables
2. Any missing value in the data must be removed or estimated.
3. The data must be standardized (i.e., scaled) to make variables comparable (I am not doing this here). 

### Read in the data

```{r}

clust_data <- read_csv(here("data", "class_wide_1.csv")) # read in data

clust_data <- select(clust_data, -X1, -`54`) # remove extra col sub 54 has weird formatting

clust_data <- as.data.frame(clust_data) # turn into df 

rownames(clust_data) <- clust_data$speaker # make row names speaker

clust_data <- select(clust_data,-speaker) # remove extra col sub 54 has weird formatting

head(clust_data)# show first couple rows

```

## Agglomerative Hierarchical Clustering

I am going to cluster the data  using average link clustering. Average link clustering computes all pairwise dissimilarities between the elements, and considers the average of these dissimilarities as the distance between clusters. 

First, we calculate the dissimilarity matrix using euclidean distance. 

Second, we comput the clustering with average link. 

Third, we plot the cluster solution

```{r}
# Dissimilarity matrix
d <- dist(clust_data, method = "euclidean")

# Hierarchical clustering using Average Linkage
hc1 <- hclust(d, method = "average" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)


```


In the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.

The height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.

The height of the cut to the dendrogram controls the number of clusters obtained. It plays the same role as the k in k-means clustering. In order to identify sub-groups (i.e. clusters), we can cut the dendrogram with cutree:

```{r}
# Ward's method
hc5 <- hclust(d, method = "average" )

# Cut tree into 3 groups
sub_grp <- cutree(hc5, k = 3)

# Number of members in each cluster
table(sub_grp)

```


### Visualize clusters on dendrogram 

```{r}
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 3, border = 2:5)

```

From this, we glean that 3 clusters appear to be adequate. Generally participants grouped speakers into 3 clusters/groups:  

- English/African: Clust 1

- Indo/European: Clust  2 

- Asian: Clust 3


```{r}

clust_data <- clust_data %>%
  mutate(cluster = sub_grp)

```

### Visualize Clusters

```{r}
fviz_cluster(list(data = clust_data, cluster = sub_grp))
```


```{r}
#making a empty dataframe
criteria <- data.frame()
#setting range of k                   
nk <- 1:20
#loop for range of clusters
for (k in nk) {
model <- kmeans(clust_data, k)
criteria <- rbind(criteria,c(k,model$tot.withinss,model$betweenss,model$totss))
}
#renaming columns
names(criteria) <- c("k","tot.withinss","betweenss","totalss")

#scree plot
ggplot(criteria, aes(x=k)) +
  geom_point(aes(y=tot.withinss),color="red") +
  geom_line(aes(y=tot.withinss),color="red") +
  geom_point(aes(y=betweenss),color="blue") +
  geom_line(aes(y=betweenss),color="blue") +
  xlab("k = number of clusters") + ylab("Sum of Squares (within = red, between = blue)")

```

# Conclusion
I ran a hierarchical clustering analysis using the average link method to classify talkers in a free classification task. This analysis determined that 3 clusters were adequate to explain the free classification task.  A common method for choosing the optimal number of clusters is the “elbow method”, which simply means picking the point on the plot where increasing the value of k results in only marginal gains. Here we have determined that 3 clusters were sufficient to explain the data. 

